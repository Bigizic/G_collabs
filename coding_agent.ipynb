{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuZOGaQqFaKpcNVNhgcilu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bigizic/get_real/blob/main/coding_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6lykQ0vjafA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pathlib openai numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAen-rw9aehC",
        "outputId": "1b665c97-20c2-41e8-eebf-9fbe488d78d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "def smart_sample_js_files(base_path: Path, inputs: List[str]):\n",
        "    print(\"Smart-sampling JS files...\")\n",
        "\n",
        "    input_paths = [(base_path / i).resolve() for i in inputs]\n",
        "    js_files = []\n",
        "\n",
        "    for path in input_paths:\n",
        "        if not path.exists():\n",
        "            print(f\"Path does not exist: {path}\")\n",
        "            continue\n",
        "\n",
        "        if path.is_file() and path.suffix in [\".js\", \".jsx\", \".ts\", \".tsx\"]:\n",
        "            js_files.append(path)\n",
        "        elif path.is_dir():\n",
        "            for f in path.rglob(\"*\"):\n",
        "                if (\n",
        "                    f.suffix in [\".js\", \".jsx\", \".ts\", \".tsx\"]\n",
        "                    and all(skip not in f.parts for skip in [\"node_modules\", \"build\", \"dist\", \"Gpt\"])\n",
        "                ):\n",
        "                    js_files.append(f)\n",
        "\n",
        "    summaries = []\n",
        "    for file_path in js_files:\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "                print(lines)\n",
        "\n",
        "            total_lines = len(lines)\n",
        "            # if total_lines <= 10000:\n",
        "            sample = lines\n",
        "            \"\"\"else:\n",
        "                sample = (\n",
        "                    lines[:40]\n",
        "                    + [\"\\n// ...\\n\"]\n",
        "                    + lines[total_lines // 2 - 15: total_lines // 2 + 15]\n",
        "                    + [\"\\n// ...\\n\"]\n",
        "                    + lines[-30:]\n",
        "                )\n",
        "            \"\"\"\n",
        "\n",
        "            summary = f\"\\n\\n// === FILE: {file_path.relative_to(base_path)} ===\\n\"\n",
        "            summary += \"\".join(sample)\n",
        "            summaries.append(summary)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {file_path}: {e}\")\n",
        "\n",
        "    print(f\"{len(summaries)} files summarized.\")\n",
        "    return \"\\n\".join(summaries)\n"
      ],
      "metadata": {
        "id": "F8tnyHtBaWTq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def read_files(paths, max_chars=15000):\n",
        "    \"\"\"\n",
        "    read project files from a list of paths and return a single string.\n",
        "    it stops once it hits max_chars to avoid sending too much to gpt.\n",
        "    \"\"\"\n",
        "    collected = []\n",
        "    total = 0\n",
        "\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        if p.is_file():\n",
        "            try:\n",
        "                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "                if total + len(text) > max_chars:\n",
        "                    text = text[: max_chars - total]\n",
        "                collected.append(f\"\\n--- file: {p} ---\\n{text}\")\n",
        "                total += len(text)\n",
        "                if total >= max_chars:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"could not read {p}: {e}\")\n",
        "        elif p.is_dir():\n",
        "            for file in p.rglob(\"*\"):\n",
        "                if file.is_file():\n",
        "                    try:\n",
        "                        text = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "                        if total + len(text) > max_chars:\n",
        "                            text = text[: max_chars - total]\n",
        "                        collected.append(f\"\\n--- file: {file} ---\\n{text}\")\n",
        "                        total += len(text)\n",
        "                        if total >= max_chars:\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"could not read {file}: {e}\")\n",
        "            if total >= max_chars:\n",
        "                break\n",
        "\n",
        "    return \"\\n\".join(collected)"
      ],
      "metadata": {
        "id": "53AFgkbOTAg1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import re\n",
        "import cmd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPEN_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OPEN_API_KEY\n",
        "client = OpenAI()\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "CHUNK_SIZE = 2000  # number of characters per chunk\n",
        "CODE_INDEX = {}    # { \"relative/path.js\": [ (chunk_text, embedding), ... ] }\n",
        "\n",
        "\n",
        "def chunk_text(text, size=CHUNK_SIZE):\n",
        "    \"\"\"split text into chunks of given size\"\"\"\n",
        "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
        "\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"generate embedding for a given string\"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "\n",
        "def index_codebase(project_path, exts=(\".js\", \".ts\", \".jsx\", \".tsx\", \".json\", \".py\")):\n",
        "    \"\"\"index all code files by splitting into chunks and embedding them\"\"\"\n",
        "    index = {}\n",
        "    project_path = Path(project_path)\n",
        "    files = [f for f in project_path.rglob(\"*\") if f.suffix in exts]\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "            chunks = chunk_text(content)\n",
        "\n",
        "            indexed_chunks = []\n",
        "            for chunk in chunks:\n",
        "                embedding = embed_text(chunk)\n",
        "                indexed_chunks.append((chunk, embedding))\n",
        "\n",
        "            index[str(file.relative_to(project_path))] = indexed_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"skipping {file}: {e}\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=10):\n",
        "    \"\"\"return the most relevant code chunks for a given query prompt\"\"\"\n",
        "    query_embedding = embed_text(query)\n",
        "    scored_chunks = []\n",
        "\n",
        "    for file, chunks in CODE_INDEX.items():\n",
        "        for chunk_text, embedding in chunks:\n",
        "            score = cosine_similarity(query_embedding, embedding)\n",
        "            scored_chunks.append((score, file, chunk_text))\n",
        "\n",
        "    # sort by score (highest first)\n",
        "    scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
        "    return scored_chunks[:top_k]\n",
        "\n",
        "\n",
        "class GptCodeAgent(cmd.Cmd):\n",
        "    intro = \"gpt project coding agent started. type 'help' for commands.\\n\"\n",
        "    prompt = \"(gpt) \"\n",
        "    use_rawinput = True\n",
        "\n",
        "    def preloop(self):\n",
        "        print(\"welcome, let's set up your coding agent.\\n\")\n",
        "\n",
        "        # project path\n",
        "        self.project_path = Path(input(\"enter path to your existing project folder: \").strip()).resolve()\n",
        "        if not self.project_path.exists():\n",
        "            print(\"invalid project path, exiting.\")\n",
        "            return self.do_exit(\"\")\n",
        "\n",
        "        # output directory\n",
        "        self.output_dir = Path(input(\"enter new project directory name (for generated files): \").strip()).resolve()\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"output will be stored in: {self.output_dir}\")\n",
        "\n",
        "        # global prompt\n",
        "        self.global_prompt = input(\"enter your global prompt (instructions for gpt):\\n\")\n",
        "\n",
        "        # build index\n",
        "        global CODE_INDEX\n",
        "        print(\"\\nindexing project files and generating embeddings...\")\n",
        "        CODE_INDEX = index_codebase(self.project_path)\n",
        "        print(f\"indexed {len(CODE_INDEX)} files.\")\n",
        "\n",
        "        # continue\n",
        "        self.ask_action()\n",
        "\n",
        "    def ask_action(self):\n",
        "        print(\"\\nwhat do you want to do?\")\n",
        "        print(\"1. implement feature across project\")\n",
        "        print(\"2. create a new file\")\n",
        "        print(\"3. update existing file\")\n",
        "        choice = input(\"enter 1, 2, or 3: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            self.project_wide_update()\n",
        "        elif choice == \"2\":\n",
        "            self.create_file()\n",
        "        elif choice == \"3\":\n",
        "            self.update_file()\n",
        "        else:\n",
        "            print(\"invalid choice.\")\n",
        "            self.ask_action()\n",
        "\n",
        "    def send_to_gpt(self, instruction, retrieved_chunks=None):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"you are a senior full-stack coding assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"global project instructions:\\n{self.global_prompt}\"}\n",
        "        ]\n",
        "\n",
        "        if retrieved_chunks:\n",
        "            for _, file, chunk in retrieved_chunks:\n",
        "                messages.append({\"role\": \"user\", \"content\": f\"file: {file}\\ncode:\\n{chunk}\"})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"instruction:\\n{instruction}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def project_wide_update(self):\n",
        "      print(\"working on full project update...\")\n",
        "      \"\"\"def project_wide_update(self):\n",
        "        feature_prompt = input(\"describe the feature you want to implement:\\n\")\n",
        "\n",
        "        relevant_chunks = retrieve_relevant_chunks(feature_prompt, top_k=10)\n",
        "        gpt_response = self.send_to_gpt(feature_prompt, retrieved_chunks=relevant_chunks)\n",
        "\n",
        "        output_file = self.output_dir / \"project_wide_changes.txt\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(gpt_response)\n",
        "        print(f\"project-wide suggestions saved to: {output_file}\")\n",
        "        self.ask_action()\n",
        "      \"\"\"\n",
        "      prompt = self.global_prompt\n",
        "\n",
        "      files_context = read_files(self.project_path)\n",
        "\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": (\n",
        "                  \"you are a senior full-stack developer. the project is a full-stack javascript web app \"\n",
        "                  \"with react (redux) frontend and express + mongodb backend. \"\n",
        "                  \"when outputting changes, you must strictly follow this format:\\n\\n\"\n",
        "                  \"file: relative/path/to/file.js\\n\"\n",
        "                  \"```js\\n\"\n",
        "                  \"// full file code here\\n\"\n",
        "                  \"```\\n\\n\"\n",
        "                  \"file: another/file.js\\n\"\n",
        "                  \"```js\\n\"\n",
        "                  \"// full file code here\\n\"\n",
        "                  \"```\\n\\n\"\n",
        "                  \"only output file blocks in this format. do not include explanations, comments, or text outside of this structure.\"\n",
        "              )\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"my project includes these files:\\n{files_context[:12000]}...\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"implement this feature:\\n\\n{prompt}\"\n",
        "          }\n",
        "      ]\n",
        "\n",
        "      response = self.client.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          messages=messages,\n",
        "          temperature=0.3\n",
        "      )\n",
        "\n",
        "      full_suggestion = response.choices[0].message.content\n",
        "\n",
        "      # regex to parse gpt output: capture file path and code inside fences\n",
        "      file_pattern = r\"file:\\s*(.+?)\\n```[a-zA-Z]*\\n(.*?)```\"\n",
        "      matches = re.findall(file_pattern, full_suggestion, re.DOTALL)\n",
        "\n",
        "      if not matches:\n",
        "          print(\"no valid file blocks detected, saving raw output...\")\n",
        "          output_file = self.output_dir / \"project_wide_changes_raw.txt\"\n",
        "          with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "              f.write(full_suggestion)\n",
        "          print(f\"raw output saved to: {output_file}\")\n",
        "          return\n",
        "\n",
        "      for rel_path, code in matches:\n",
        "          rel_path = rel_path.strip()\n",
        "          output_file = self.output_dir / rel_path\n",
        "          output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "          with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "              f.write(code.strip() + \"\\n\")\n",
        "          print(f\"wrote changes to {output_file}\")\n",
        "\n",
        "      print(\"project-wide update complete\")\n",
        "      self.ask_action()\n",
        "\n",
        "    def create_file(self):\n",
        "        file_name = input(\"enter new file path (relative to project): \").strip()\n",
        "        prompt = input(\"describe what the file should contain:\\n\")\n",
        "        gpt_response = self.send_to_gpt(prompt)\n",
        "\n",
        "        output_file = self.output_dir / file_name\n",
        "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(gpt_response)\n",
        "        print(f\"new file created at: {output_file}\")\n",
        "        self.ask_action()\n",
        "\n",
        "    def update_file(self):\n",
        "        rel_path = input(\"enter file path (relative to project): \").strip()\n",
        "        if rel_path not in CODE_INDEX:\n",
        "            print(\"file not found in index.\")\n",
        "            return self.ask_action()\n",
        "\n",
        "        prompt = input(\"enter update instruction:\\n\")\n",
        "\n",
        "        # retrieve most relevant chunks from this specific file\n",
        "        file_chunks = CODE_INDEX[rel_path]\n",
        "        query_embedding = embed_text(prompt)\n",
        "        scored = [(cosine_similarity(query_embedding, emb), chunk) for chunk, emb in file_chunks]\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        best_chunks = [(\"file\", rel_path, chunk) for _, chunk in scored[:3]]\n",
        "\n",
        "        gpt_response = self.send_to_gpt(prompt, retrieved_chunks=best_chunks)\n",
        "\n",
        "        output_file = self.output_dir / rel_path\n",
        "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(gpt_response)\n",
        "        print(f\"updated file saved at: {output_file}\")\n",
        "        self.ask_action()\n",
        "\n",
        "    def do_exit(self, arg):\n",
        "        print(\"goodbye\")\n",
        "        return True\n",
        "\n",
        "\n",
        "def start():\n",
        "    cli = GptCodeAgent()\n",
        "    cli.cmdloop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start()\n"
      ],
      "metadata": {
        "id": "Ax2xDQuy09vJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "97eb43df-7d39-4f26-8ab1-f82aa42802a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome, let's set up your coding agent.\n",
            "\n",
            "enter path to your existing project folder: TLH/server\n",
            "enter new project directory name (for generated files): niox1\n",
            "output will be stored in: /content/niox1\n",
            "enter your global prompt (instructions for gpt):\n",
            "create this models i've structured here: {   \"nioxDB\": [     {       \"user\": {         \"name\": [           {             \"type\": \"String\",             \"required\": true           }         ],         \"phone_number\": [           {             \"type\": \"String\",             \"required\": true           }         ],         \"email\": [           {             \"type\": \"String\",             \"unique\": true,             \"required\": true           }         ],         \"password\": [           {             \"type\": \"String\",             \"require\": true,             \"select\": false           }         ],         \"google_id\": [           {             \"type\": \"String\",             \"default\": null           }         ],         \"apple_id\": [           {             \"type\": \"String\",             \"default\": null           }         ],         \"facebook_id\": [           {             \"type\": \"String\",             \"default\": null           }         ],         \"otp_code\": {           \"type\": \"Integer\"         },         \"has_complete_verification\": [           {             \"type\": \"Boolean\",             \"default\": false           }         ],         \"otp_expires\": {           \"type\": \"DateTime Object\"         },         \"reset_password_token\": [           {             \"type\": \"String\",             \"unique\": true           }         ],         \"reset_password_expires\": {           \"type\": \"DateTime Object\"         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"package\": {         \"name\": {           \"type\": \"String\"         },         \"slug\": {           \"type\": \"String\"         },         \"description\": {           \"type\": \"String\"         },         \"price\": {           \"type\": \"Integer\"         },         \"priority_price\": {           \"type\": \"Object\"         },         \"discount\": {           \"type\": \"Boolean\",           \"default\": false         },         \"discount_price\": {           \"type\": \"Integer\",           \"default\": 0         },         \"currency\": {           \"type\": \"String\",           \"default\": \"NGN\"         },         \"delivery_time\": {           \"type\": \"Integer\"         },         \"is_active\": {           \"type\": \"Boolean\",           \"default\": true         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"card\": {         \"user_id\": {           \"type\": \"String\",           \"required\": true         },         \"email\": {           \"type\": \"String\",           \"required\": true         },         \"data\": {           \"type\": \"Object\",           \"required\": true         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"cart\": {         \"owner\": {           \"user\": {             \"_id\": \"String\"           }         },         \"package\": {           \"type\": \"String\"         },         \"amount\": {           \"type\": \"String\"         },         \"discount\": {           \"type\": \"Boolean\",           \"default\": false         },         \"discount_amount\": {           \"type\": \"Integer\",           \"default\": 0         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"order\": {         \"owner\": {           \"type\": \"String\",           \"required\": true,           \"user\": {             \"type\": \"String (uuid)\",             \"name\": \"String (user name)\",             \"phone_number\": \"String\",             \"unique\": true           }         },         \"cart\": {           \"type\": \"String\"         },         \"amount\": {           \"type\": \"Integer\"         },         \"discount\": {           \"type\": \"Boolean\",           \"default\": false         },         \"discount_amount\": {           \"type\": \"Integer\",           \"default\": 0         },         \"amount_before_discount\": {           \"type\": \"Integer\",           \"default\": 0         },          \"payment_status\": {           \"type\": \"String\"         },         \"payment_id\": {           \"type\": \"String\"         },         \"payment_fees\": {           \"type\": \"Integer\",           \"default\": 0         },         \"payment_reference\": {           \"type\": \"String\"         },         \"payment_method\": {           \"type\": \"String\"         },         \"status\": {           \"type\": \"Boolean\",           \"default\": false         },         \"billingEmail\": {           \"type\": \"String\"         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"website\": {         \"order_id\": [           {             \"type\": \"String\",             \"required\": true           }         ],         \"domain\": {           \"type\": \"String\"         },         \"status\": [           {             \"type\": \"String\",             \"enum\": [               \"development\",               \"testing\",               \"live\"             ]           }         ],         \"note\": {           \"type\": \"String\"         },         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     },     {       \"review\": {         \"user_id\": \"id\",         \"title\": \"\",         \"created_at\": {           \"type\": \"DateTime Object\"         },         \"updated_at\": {           \"type\": \"DateTime Object\"         }       }     }   ] }\n",
            "\n",
            "indexing project files and generating embeddings...\n",
            "indexed 81 files.\n",
            "\n",
            "what do you want to do?\n",
            "1. implement feature across project\n",
            "2. create a new file\n",
            "3. update existing file\n",
            "enter 1, 2, or 3: 1\n",
            "working on full project update...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'PosixPath' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4263711151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4263711151.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mcli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGptCodeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/cmd.py\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletekey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4263711151.py\u001b[0m in \u001b[0;36mpreloop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mask_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4263711151.py\u001b[0m in \u001b[0;36mask_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_wide_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4263711151.py\u001b[0m in \u001b[0;36mproject_wide_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m       \u001b[0mfiles_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       messages = [\n",
            "\u001b[0;32m/tmp/ipython-input-2830175051.py\u001b[0m in \u001b[0;36mread_files\u001b[0;34m(paths, max_chars)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'PosixPath' object is not iterable"
          ]
        }
      ]
    }
  ]
}