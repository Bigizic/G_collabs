{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV3JTMn2JbPm7c8HPZyag4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bigizic/get_real/blob/main/coding_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6lykQ0vjafA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pathlib openai numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAen-rw9aehC",
        "outputId": "353a5056-7c1e-434f-aef4-38a5ecbf2de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def read_files(paths, max_chars=15000):\n",
        "    \"\"\"\n",
        "    read project files from a list of paths and return a single string.\n",
        "    it stops once it hits max_chars to avoid sending too much to gpt.\n",
        "    \"\"\"\n",
        "    collected = []\n",
        "    total = 0\n",
        "\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        if p.is_file():\n",
        "            try:\n",
        "                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "                if total + len(text) > max_chars:\n",
        "                    text = text[: max_chars - total]\n",
        "                collected.append(f\"\\n--- file: {p} ---\\n{text}\")\n",
        "                total += len(text)\n",
        "                if total >= max_chars:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"could not read {p}: {e}\")\n",
        "        elif p.is_dir():\n",
        "            for file in p.rglob(\"*\"):\n",
        "                if file.is_file():\n",
        "                    try:\n",
        "                        text = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "                        if total + len(text) > max_chars:\n",
        "                            text = text[: max_chars - total]\n",
        "                        collected.append(f\"\\n--- file: {file} ---\\n{text}\")\n",
        "                        total += len(text)\n",
        "                        if total >= max_chars:\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"could not read {file}: {e}\")\n",
        "            if total >= max_chars:\n",
        "                break\n",
        "\n",
        "    return \"\\n\".join(collected)"
      ],
      "metadata": {
        "id": "53AFgkbOTAg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import re\n",
        "import cmd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPEN_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OPEN_API_KEY\n",
        "client = OpenAI()\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "CHUNK_SIZE = 2000  # number of characters per chunk\n",
        "CODE_INDEX = {}    # { \"relative/path.js\": [ (chunk_text, embedding), ... ] }\n",
        "\n",
        "\n",
        "def chunk_text(text, size=CHUNK_SIZE):\n",
        "    \"\"\"split text into chunks of given size\"\"\"\n",
        "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
        "\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"generate embedding for a given string\"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return np.array(response.data[0].embedding)\n",
        "\n",
        "\n",
        "def index_codebase(project_path, exts=(\".js\", \".ts\", \".jsx\", \".tsx\", \".json\", \".py\")):\n",
        "    \"\"\"index all code files by splitting into chunks and embedding them\"\"\"\n",
        "    index = {}\n",
        "    project_path = Path(project_path)\n",
        "    files = None\n",
        "    if project_path.is_file():\n",
        "      files = [project_path]\n",
        "    else:\n",
        "      files = [f for f in project_path.rglob(\"*\") if f.suffix in exts]\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "            chunks = chunk_text(content)\n",
        "\n",
        "            indexed_chunks = []\n",
        "            for chunk in chunks:\n",
        "                embedding = embed_text(chunk)\n",
        "                indexed_chunks.append((chunk, embedding))\n",
        "\n",
        "            index[str(file.relative_to(project_path))] = indexed_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"skipping {file}: {e}\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=10):\n",
        "    \"\"\"return the most relevant code chunks for a given query prompt\"\"\"\n",
        "    query_embedding = embed_text(query)\n",
        "    scored_chunks = []\n",
        "\n",
        "    for file, chunks in CODE_INDEX.items():\n",
        "        for chunk_text, embedding in chunks:\n",
        "            score = cosine_similarity(query_embedding, embedding)\n",
        "            scored_chunks.append((score, file, chunk_text))\n",
        "\n",
        "    # sort by score (highest first)\n",
        "    scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
        "    return scored_chunks[:top_k]\n",
        "\n",
        "\n",
        "class GptCodeAgent(cmd.Cmd):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.project_path = None\n",
        "        self.output_dir = None\n",
        "        self.project_path = None\n",
        "        self.use_rawinput = True\n",
        "\n",
        "        self.chat_history = []\n",
        "\n",
        "        self.intro = \"Coding agent started. type 'help' for commands.\\n\"\n",
        "        self.global_prompt = \"\"\n",
        "        self.prompt = \"Set up your coding agent. Lets get started.\\n\"\n",
        "        self.prompter = \"Select: \"\n",
        "        self.prompt1 = \"1. Import codebase\"\n",
        "        self.prompt2 = \"2. Chat with coding agent\"\n",
        "        self.prompt3 = \"Enter path to codebase or path to file: \"\n",
        "        self.prompt4 = (\"Enter a new project directory to save changes in a separate\\n\"\n",
        "                         \"location, or press enter to save changes directly in\\n\"\n",
        "                         \"the existing codebase:\\n\")\n",
        "\n",
        "        self.prompt5 = \"Select 1 to continue working on project\\n\"\n",
        "        self.prompt6 = \"Select 2 to choose from a new file or directory\"\n",
        "\n",
        "    def preloop(self):\n",
        "        print(self.intro)\n",
        "        print(self.prompt)\n",
        "        print(self.prompt1)\n",
        "        print(self.prompt2)\n",
        "        choice = input(self.prompter).strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            self.import_codebase()\n",
        "        elif choice == \"2\":\n",
        "            self.chat()\n",
        "        else:\n",
        "            print(\"invalid choice.\")\n",
        "            self.preloop()\n",
        "\n",
        "    def import_codebase(self, should_continue_chat=False):\n",
        "        entry = False\n",
        "\n",
        "        if should_continue_chat:\n",
        "          print(\"Select a new file or directory or continue to work on current project\\n\")\n",
        "          print(self.prompt5)\n",
        "          print(self.prompt6)\n",
        "          continue_chat_choice = input(self.prompter)\n",
        "          if continue_chat_choice == \"1\":\n",
        "            return self.project_wide_update()\n",
        "          elif continue_chat_choice == \"2\":\n",
        "            entry = True\n",
        "        else:\n",
        "          entry = True\n",
        "\n",
        "        if entry:\n",
        "          # project path\n",
        "          self.project_path = Path(input(self.prompt3).strip()).resolve()\n",
        "          if not self.project_path.exists():\n",
        "            print(\"invalid project path, exiting.\")\n",
        "            return self.do_exit(\"\")\n",
        "\n",
        "          # build index\n",
        "          global CODE_INDEX\n",
        "          print(\"\\nindexing project files and generating embeddings...\")\n",
        "          CODE_INDEX = index_codebase(self.project_path)\n",
        "          print(f\"indexed {len(CODE_INDEX)} files.\\n\")\n",
        "\n",
        "          # output directory\n",
        "          user_input = input(self.prompt4).strip()\n",
        "          self.output_dir = Path(user_input).resolve() if user_input else self.project_path\n",
        "          if user_input and user_input.len() > 0:\n",
        "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "          print(f\"Output will be stored in: {self.output_dir}\")\n",
        "          self.project_wide_update()\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"continue chatting with gpt to edit or create files\"\"\"\n",
        "        user_prompt = input(\"enter your instruction:\\n\")\n",
        "\n",
        "        # add user message to history\n",
        "        self.chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "        messages = [{\"role\": \"system\", \"content\": \"you are a senior full-stack coding assistant.\"}]\n",
        "        # include global prompt at the start if set\n",
        "        if self.global_prompt:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"global project instructions:\\n{self.global_prompt}\"})\n",
        "        # include history\n",
        "        messages.extend(self.chat_history)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        gpt_output = response.choices[0].message.content\n",
        "        self.chat_history.append({\"role\": \"assistant\", \"content\": gpt_output})\n",
        "\n",
        "        # try parsing file blocks like project_wide_update\n",
        "        file_pattern = r\"file:\\s*(.+?)\\n```[a-zA-Z]*\\n(.*?)```\"\n",
        "        matches = re.findall(file_pattern, gpt_output, re.DOTALL)\n",
        "\n",
        "        if not matches:\n",
        "            # fallback: save whole thing in raw\n",
        "            output_file = self.output_dir / \"chat_output_raw.txt\"\n",
        "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(gpt_output + \"\\n\\n\")\n",
        "            print(f\"raw chat output saved to {output_file}\")\n",
        "        else:\n",
        "            for rel_path, code in matches:\n",
        "                rel_path = rel_path.strip()\n",
        "                # if model gives full path like project/foo.js, use it\n",
        "                # if not, fallback to rel_path as is\n",
        "                file_path = rel_path.split('/')[2:] or [rel_path]\n",
        "                output_file = self.output_dir / \"/\".join(file_path)\n",
        "                output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(code.strip() + \"\\n\")\n",
        "                print(f\"wrote changes to {output_file}\")\n",
        "\n",
        "        self.chat()\n",
        "\n",
        "    def send_to_gpt(self, instruction, retrieved_chunks=None):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"you are a senior full-stack coding assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"global project instructions:\\n{self.global_prompt}\"}\n",
        "        ]\n",
        "\n",
        "        if retrieved_chunks:\n",
        "            for _, file, chunk in retrieved_chunks:\n",
        "                messages.append({\"role\": \"user\", \"content\": f\"file: {file}\\ncode:\\n{chunk}\"})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"instruction:\\n{instruction}\"})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def project_wide_update(self):\n",
        "      # global prompt\n",
        "      self.global_prompt = input(\"\\nEnter your prompt:\\n\")\n",
        "      print(\"\\nworking on full project update...\")\n",
        "\n",
        "      files_context = read_files([self.project_path])\n",
        "\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": (\n",
        "                  \"you are a senior full-stack developer. the project is a full-stack javascript web app \"\n",
        "                  \"with react (redux) frontend and express + mongodb backend. \"\n",
        "                  \"when outputting changes, you must strictly follow this format:\\n\\n\"\n",
        "                  \"file: path/to/file.js\\n\"\n",
        "                  \"```js\\n\"\n",
        "                  \"// full file code here\\n\"\n",
        "                  \"```\\n\\n\"\n",
        "                  \"file: another/file.js\\n\"\n",
        "                  \"```js\\n\"\n",
        "                  \"// full file code here\\n\"\n",
        "                  \"```\\n\\n\"\n",
        "                  \"only output file blocks in this format. do not include explanations, comments, or text outside of this structure.\"\n",
        "              )\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"my project includes these files:\\n{files_context[:12000]}...\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"implement this feature:\\n\\n{self.global_prompt}\"\n",
        "          }\n",
        "      ]\n",
        "\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          messages=messages,\n",
        "          temperature=0.3\n",
        "      )\n",
        "\n",
        "      full_suggestion = response.choices[0].message.content\n",
        "\n",
        "      # regex to parse gpt output: capture file path and code inside fences\n",
        "      file_pattern = r\"file:\\s*(.+?)\\n```[a-zA-Z]*\\n(.*?)```\"\n",
        "      matches = re.findall(file_pattern, full_suggestion, re.DOTALL)\n",
        "\n",
        "      if not matches:\n",
        "          print(\"no valid file blocks detected, saving raw output...\")\n",
        "          output_file = self.output_dir / \"project_wide_changes_raw.txt\"\n",
        "          with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "              f.write(full_suggestion)\n",
        "          print(f\"raw output saved to: {output_file}\")\n",
        "          return\n",
        "\n",
        "      for rel_path, code in matches:\n",
        "          rel_path = rel_path.strip()\n",
        "          output_file = \"\"\n",
        "          if self.output_dir == self.project_path:  # edit file or files in project here\n",
        "            print(rel_path)\n",
        "            # file_path = rel_path.split('/')[-1]\n",
        "            # output_file = self.output_dir / file_path if self.output_dir != rel_\n",
        "            output_file = rel_path\n",
        "          else:\n",
        "            file_path = rel_path.split('/')[2:]\n",
        "            output_file = self.output_dir / \"/\".join(file_path)\n",
        "            output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "          with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "              f.write(code.strip() + \"\\n\")\n",
        "          print(f\"\\nwrote changes to {output_file}\")\n",
        "      self.import_codebase(True)\n",
        "\n",
        "    def do_exit(self, arg):\n",
        "        print(\"goodbye\")\n",
        "        return True\n",
        "\n",
        "\n",
        "def start():\n",
        "    cli = GptCodeAgent()\n",
        "    cli.cmdloop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start()\n"
      ],
      "metadata": {
        "id": "Ax2xDQuy09vJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02f2728d-239a-41d5-b797-a2a6fa543860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coding agent started. type 'help' for commands.\n",
            "\n",
            "Set up your coding agent. Lets get started.\n",
            "\n",
            "1. Import codebase\n",
            "2. Chat with coding agent\n",
            "Select: 1\n",
            "Enter path to codebase or path to file: /content/niox/TLH/server/models/cart.js\n",
            "\n",
            "indexing project files and generating embeddings...\n",
            "indexed 1 files.\n",
            "\n",
            "Enter a new project directory to save changes in a separate\n",
            "location, or press enter to save changes directly in\n",
            "the existing codebase:\n",
            "\n",
            "Output will be stored in: /content/niox/TLH/server/models/cart.js\n",
            "\n",
            "Enter your prompt:\n",
            "the _id field of the user here  in the cart should be of type ObjectId and it should ref to 'User'\n",
            "\n",
            "working on full project update...\n",
            "/content/niox/TLH/server/models/cart.js\n",
            "\n",
            "wrote changes to /content/niox/TLH/server/models/cart.js\n",
            "Select a new file or directory or continue to work on current project\n",
            "Select 1 to continue working on project\n",
            "\n",
            "Select 2 to choose from a new file or directory\n",
            "Select: 2\n",
            "Enter path to codebase or path to file: /content/niox/TLH/server/models/order.js\n",
            "\n",
            "indexing project files and generating embeddings...\n",
            "indexed 1 files.\n",
            "\n",
            "Enter a new project directory to save changes in a separate\n",
            "location, or press enter to save changes directly in\n",
            "the existing codebase:\n",
            "\n",
            "Output will be stored in: /content/niox/TLH/server/models/order.js\n",
            "\n",
            "Enter your prompt:\n",
            "do the same for the order to, the user _id something like earlier\n",
            "\n",
            "working on full project update...\n",
            "/content/niox/TLH/server/models/order.js\n",
            "\n",
            "wrote changes to /content/niox/TLH/server/models/order.js\n",
            "Select a new file or directory or continue to work on current project\n",
            "Select 1 to continue working on project\n",
            "\n",
            "Select 2 to choose from a new file or directory\n",
            "Select: collect name and phone number for the user\n",
            "Coding agent started. type 'help' for commands.\n",
            "\n",
            "Set up your coding agent. Lets get started.\n",
            "1\n",
            "*** Unknown syntax: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-627897788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-627897788.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mcli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGptCodeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/cmd.py\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"/content/niox\", 'zip', \"/content/niox\")\n",
        "\n",
        "files.download(\"/content/niox.zip\")\n"
      ],
      "metadata": {
        "id": "bKMMwyaGsQ6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}